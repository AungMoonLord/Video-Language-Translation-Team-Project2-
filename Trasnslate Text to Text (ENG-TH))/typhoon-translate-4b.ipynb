{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f2d427",
   "metadata": {},
   "source": [
    "โมเดล scb10x/typhoon-translate-4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b06674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac08cd7e25434bccb6178e88794e33bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สุนัขขี้เกียจถูกกระโดดข้ามโดยสุนัขสีน้ำตาลตัวเร็ว\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"scb10x/typhoon-translate-4b\"\n",
    "\n",
    "# โหลด Tokenizer และ Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\" # หรือ device_map={\"\": 0} เพื่อบังคับใช้ GPU\n",
    ")\n",
    "\n",
    "# ข้อความที่ต้องการแปล\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# สำคัญ: ต้องใช้ System Prompt ตามที่โมเดลกำหนดเพื่อให้ผลลัพธ์แม่นยำ\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following text into Thai.\"},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "# เตรียม Input\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# สั่งให้โมเดล Generate ผลลัพธ์\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=False # แนะนำให้ปิด sampling เพื่อความแม่นยำในการแปล\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c439b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph1 = \"Ais found herself in darkness, not knowing where she was or why she was there. As she looked around, she realized she had transformed back into her younger self, and she felt lonely. Suddenly, her childhood toys appeared as her friends: sister dolls, a plush wolf, a soldier, a light elf, and a cat-shaped soldier, all offering her companionship. Feeling brave, Ais declared herself an adventurer, picked up a toy sword and shield, and began her adventure with her friends.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01471274",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph2 = \"Together, they faced a dim labyrinth filled with monsters, but Ais felt unafraid. With the help of her small friends, they defeated the monsters one by one. Fairies illuminated their path with colorful magic. Ais swung her sword and managed to defeat the monsters, leading her friends in joyful song and dance as they marched deeper into the labyrinth.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b18cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph3 = \"They descended deeper and deeper, down sixty levels, until a cold wind blew. Something wet and sticky splattered on Ais, and when she opened her eyes, she found silence. Turning around, she was horrified to see a crimson spring where her friends lay twisted and injured. Their arms and legs were disfigured, and some were beyond recognition, their stuffing spilling out like entrails. The warm substance on Ais was revealed to be blood — her friends' blood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973972bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไอส์พบว่าตัวเองอยู่ในความมืด ไม่รู้ว่าตัวเองอยู่ที่ไหนหรือทำไมถึงมาอยู่ที่นี่ ขณะที่เธอหันมองไปรอบๆ เธอตระหนักได้ว่าตัวเองได้เปลี่ยนกลับไปเป็นเด็กน้อยอีกครั้ง และเธอรู้สึกโดดเดี่ยว ทันใดนั้นของเล่นในวัยเด็กของเธอก็ปรากฏตัวขึ้นเป็นเพื่อนๆ ได้แก่ ตุ๊กตาพี่สาว หมาป่าตุ๊กตาหมีนุ่ม ทหารหุ่นยนต์ เอลฟ์แสง และทหารรูปแมว ทั้งหมดต่างมอบมิตรภาพให้เธอ ด้วยความกล้าหาญ ไอส์ประกาศตัวเองเป็นนักผจญภัย หยิบดาบและโล่ของเล่นขึ้นมา แล้วเริ่มต้นการผจญภัยพร้อมกับเพื่อนๆ\n",
      "\n",
      "พวกเขาเผชิญหน้ากับเขาวงกตสลัวที่เต็มไปด้วยสัตว์ประหลาด แต่ไอส์ไม่รู้สึกหวาดกลัว ด้วยความช่วยเหลือจากเพื่อนตัวเล็กๆ ของเธอ พวกเขาเอาชนะสัตว์ประหลาดไปทีละตัว เหล่าภูตผีส่องแสงนำทางด้วยเวทมนตร์สีสันสดใส ไอส์ฟันดาบและสามารถเอาชนะสัตว์ประหลาดได้ นำเพื่อนๆ ไปพร้อมกับร้องเพลงและเต้นรำอย่างมีความสุขขณะเดินลึกเข้าไปในเขาวงกต\n",
      "\n",
      "พวกเขาลงไปลึกขึ้นเรื่อยๆ ถึงชั้นที่หกสิบ จนกระทั่งมีลมเย็นพัดมา บางสิ่งเปียกและเหนียวเหนอะกระเด็นใส่ไอส์ และเมื่อเธอเปิดตาขึ้น เธอก็พบว่าความเงียบสงัด เมื่อหันกลับไป เธอหวาดกลัวที่เห็นน้ำพุสีแดงสดที่เพื่อนๆ ของเธอนอนคดเคี้ยวและบาดเจ็บ แขนขาของพวกเขาผิดรูปไปหมด บางคนไม่สามารถจำได้อีกต่อไป ขนตุ๊กตาหลุดออกมาเหมือนเครื่องในของสัตว์ เนื้ออุ่นที่อยู่บนตัวไอส์ปรากฏว่าเป็นเลือด — เลือดของเพื่อนๆ ของเธอ\n"
     ]
    }
   ],
   "source": [
    "# ข้อความที่ต้องการแปล\n",
    "text3 = paragraph1 + paragraph2 + paragraph3\n",
    "\n",
    "# สำคัญ: ต้องใช้ System Prompt ตามที่โมเดลกำหนดเพื่อให้ผลลัพธ์แม่นยำ\n",
    "messages3 = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following text into Thai.\"},\n",
    "    {\"role\": \"user\", \"content\": text3},\n",
    "]\n",
    "\n",
    "# เตรียม Input\n",
    "input_ids3 = tokenizer.apply_chat_template(\n",
    "    messages3, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# สั่งให้โมเดล Generate ผลลัพธ์\n",
    "outputs3 = model.generate(\n",
    "    input_ids3, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=False # แนะนำให้ปิด sampling เพื่อความแม่นยำในการแปล\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs3[0][len(input_ids3[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ac9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LN_part1 = \"\"\"Volume 21\n",
    "Prologue: The World's Bizarre Theater\n",
    "Ais found herself in darkness, not knowing where she was or why she was there. As she looked around, she realized she had transformed back into her younger self, and she felt lonely. Suddenly, her childhood toys appeared as her friends: sister dolls, a plush wolf, a soldier, a light elf, and a cat-shaped soldier, all offering her companionship. Feeling brave, Ais declared herself an adventurer, picked up a toy sword and shield, and began her adventure with her friends.\n",
    "\n",
    "Together, they faced a dim labyrinth filled with monsters, but Ais felt unafraid. With the help of her small friends, they defeated the monsters one by one. Fairies illuminated their path with colorful magic. Ais swung her sword and managed to defeat the monsters, leading her friends in joyful song and dance as they marched deeper into the labyrinth.\n",
    "\n",
    "They descended deeper and deeper, down sixty levels, until a cold wind blew. Something wet and sticky splattered on Ais, and when she opened her eyes, she found silence. Turning around, she was horrified to see a crimson spring where her friends lay twisted and injured. Their arms and legs were disfigured, and some were beyond recognition, their stuffing spilling out like entrails. The warm substance on Ais was revealed to be blood — her friends' blood.\n",
    "\n",
    "Panicking, Ais froze as a chilling realization set in. The blade she held was stained red, indicating her involvement in this horror. A malicious wind laughed, having stolen Ais' voice, and the laughter grew louder, culminating in a storm that swept everything away except Ais. Her friends were lifted high and burst apart, showering Ais with a horrific rain of blood and flesh.\n",
    "\n",
    "As Ais' body became drenched in crimson, she felt as if something within her had broken. Her scream of terror and despair echoed through the darkness. She collapsed to the ground, clutching her head, while ice pillars erupted around her, trapping her heart. These pillars served as both a prison and a protective shell for her shattered heart.\n",
    "\n",
    "A malicious presence, overwhelming and monstrous, looked down at her from above. It revelled in the nightmare, finding ecstasy in Ais' suffering while its tongue slithered across the bars of her icy prison. Ais was left amidst the horror, unable to escape the dark fate that awaited her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เล่มที่ 21  \n",
      "บทนำ: โรงละครประหลาดของโลก  \n",
      "\n",
      "ไอส์พบว่าตัวเองอยู่ในความมืด ไม่รู้ว่าตัวเองอยู่ที่ไหนหรือทำไมเธอถึงมาอยู่ที่นี่ เมื่อเธอหันไปมองรอบๆ เธอตระหนักได้ว่าตัวเองได้เปลี่ยนกลับไปเป็นเด็กน้อยอีกครั้ง และเธอรู้สึกโดดเดี่ยว ทันใดนั้นของเล่นในวัยเด็กของเธอก็ปรากฏตัวขึ้นเป็นเพื่อนๆ ได้แก่ ตุ๊กตาพี่น้อง หมาป่าตุ๊กตาหมีนุ่ม ทหารหุ่นยนต์ เอลฟ์แสง และทหารรูปแมว ทั้งหมดต่างเสนอความช่วยเหลือและมิตรภาพให้เธอ ด้วยความกล้าหาญ ไอส์ประกาศตัวเองเป็นนักผจญภัย หยิบดาบและโล่ของเล่นขึ้นมา แล้วเริ่มต้นการผจญภัยพร้อมกับเพื่อนๆ ของเธอ  \n",
      "\n",
      "พวกเขาเผชิญหน้ากับเขาวงกตสลัวที่เต็มไปด้วยสัตว์ประหลาด แต่ไอส์ไม่รู้สึกหวาดกลัวเลย ด้วยความช่วยเหลือจากเพื่อนตัวเล็กๆ ของเธอ พวกเขาเอาชนะสัตว์ประหลาดไปทีละตัวได้ เทฟางก์ส่องแสงนำทางพวกเขาด้วยเวทมนตร์สีสันสดใส ไอส์ฟันดาบของเธอและเอาชนะสัตว์ประหลาดได้สำเร็จ นำเพื่อนๆ ของเธอร้องเพลงและเต้นรำอย่างมีความสุขขณะเดินลึกเข้าไปในเขาวงกต  \n",
      "\n",
      "พวกเขาลงไปลึกขึ้นเรื่อยๆ ถึงชั้นที่ 60 จนกระทั่งลมเย็นพัดมา บางสิ่งเปียกและเหนียวเหนอะกระเด็นใส่ไอส์ เมื่อเธอเปิดตาขึ้นมา เธอก็พบว่าความเงียบสงัด เมื่อหันกลับไปมอง เธอรู้สึกหวาดกลัวเมื่อเห็นน้ำพุสีแดงสดที่เพื่อนๆ ของเธอนอนคดเคี้ยวและได้รับบาดเจ็บ แขนขาของพวกเขาผิดรูปไปหมด บางคนจนจำไม่ได้ สารอัดลมภายในของพวกเขากระเด็นออกมาเหมือนเครื่องใน สารอุ่นที่อยู่บนตัวไอส์ปรากฏว่าเป็นเลือด—เลือดของเพื่อนๆ ของเธอ  \n",
      "\n",
      "ไอส์ตกใจจนตัวแข็งทื่อเมื่อเกิดความคิดที่น่าสยดสยองขึ้นมา ใบมีดที่เธอถืออยู่นั้นเปื้อนเลือด บ่งบอกถึงการมีส่วนร่วมของเธอในความสยดสยองนี้ สายลมชั่วร้ายหัว\n"
     ]
    }
   ],
   "source": [
    "# ข้อความที่ต้องการแปล\n",
    "text4 = LN_part1\n",
    "\n",
    "# สำคัญ: ต้องใช้ System Prompt ตามที่โมเดลกำหนดเพื่อให้ผลลัพธ์แม่นยำ\n",
    "messages4 = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following text into Thai.\"},\n",
    "    {\"role\": \"user\", \"content\": text4},\n",
    "]\n",
    "\n",
    "# เตรียม Input\n",
    "input_ids4 = tokenizer.apply_chat_template(\n",
    "    messages4, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# สั่งให้โมเดล Generate ผลลัพธ์\n",
    "outputs4 = model.generate(\n",
    "    input_ids4, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=False # แนะนำให้ปิด sampling เพื่อความแม่นยำในการแปล\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs4[0][len(input_ids4[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LN_part2 = \"\"\"Chapter 1: Ais SOS\n",
    "Loki Familia's expedition has failed, and despair filled the air as reports confirmed their devastating loss on the 60th Floor. Adventurers and observers alike were struck with shock and terror as the news spread throughout the city, overwhelming them with grief and disbelief. The sight of fallen comrades, bloodied and injured, painted a grim picture in the minds of those who witnessed it.\n",
    "\n",
    "Amidst the chaos, Raul, clutching his injured arm, insisted that they must help their comrades still trapped in the Deep Floors. The realization that brave adventurers had not returned, including healers and support units, ignited anger and sorrow. The crowd reacted with cries of rage and sorrow, and many were unable to process the reality of the situation, crumbling under the weight of despair.\n",
    "\n",
    "As chaos erupted in Orario, Bell realized that he couldn't find Ais among the returned, causing his heart to race, and despite knowing the circumstances, he shouted her name in desperation. Just as he prepared to run forward, Eina held him back, urging him to escape the suffocating chaos.\n",
    "\n",
    "Despite the despair enveloping the city, Eina and the rest of the Guild staff tried to maintain order and care for those who did return. Bell, feeling overwhelmed, wrestled with his emotions as he witnessed Eina's own struggle with fear and sadness. He longed for the reassurance of his comrades, Finn and others, while battling the reality of their absence.\n",
    "\n",
    "Leon placed a reassuring hand on Bell's shoulder, encouraging him to remain calm, impressing the weight of responsibility upon him. Eina remained a beacon of fragile strength, offering support despite her own terror; Bell felt a deep connection with her, echoing her apologies for the situation they faced.\n",
    "\n",
    "As healers rushed in to assist those wounded, the scene grew increasingly somber. The cries for justice and anger from earlier echoed hollow in the face of the injured being carried away. Finally, Hestia, and the rest of Hestia Familia reunited with Bell, bringing a sense of urgency and concern amidst the unfolding disaster.\n",
    "\n",
    "The survivors from the “Expedition” were taken to a special treatment facility run by the Dian Cecht Familia. In a hospital room were the Hestia Familia members, Leon, and Nina—except for Eina, who was on Guild duty. Lili looked tense, Mikoto kept her eyes lowered, and Haruhime tried to comfort them. Welf appeared calm next to Leon, though he was likely suppressing fear. Hestia’s mention of the \"60th floor\" made everyone uneasy.\n",
    "\n",
    "Leon urged everyone to stay calm, but news from the 60th floor had us on edge. A monster had appeared, and Loki, the goddess of the Loki Familia, arrived looking distressed. She confirmed that the Dungeon Boss was gone and the “Deep Floors” had collapsed, leaving a powerful monster running wild below. Loki’s expression showed both anger and pain as she confirmed that her children were still trapped in the Dungeon—and that they needed to be rescued immediately.\n",
    "\n",
    "Bell's heart raced with fear and hope when he realized that Ais and the others were still alive, as their blessings had not diminished. The other gods could sense their children through the divine blood linked to their Falna, which granted them those blessings. This realization reignited Bell's determination to help, and he urgently begged Loki to let him join the rescue team.\n",
    "\n",
    "Loki seemed exasperated by his eagerness—though she had come to them seeking help. There was a moment of levity when Hestia and Syr playfully teased Loki, easing some of the tension in the room. Leon placed a comforting hand on Bell's shoulder, offering encouragement amidst the seriousness of the situation. Then Hedin asserted himself, announcing that he would lead the rescue operation as commander and that everyone would come under his command.\n",
    "\n",
    "He had the experience needed to manage a large-scale mission, given his past leadership during the Familia War. Despite the heavy atmosphere, Bell felt honored to be among the first-class adventurers called upon for the mission. He asked Hedin what he could do to help, but to his surprise, Hedin told him to go to sleep. Bell was confused—how could he rest when the situation was so urgent?\n",
    "\n",
    "Despite his protests, Hedin insisted that Bell recover from the fatigue he had built up during their recent trials in the Valley. Then, using magic, Hedin forcefully put Bell to sleep. Bell lost consciousness after being struck by Hedin's magic.\n",
    "\n",
    "Bell collapsed onto the floor, prompting Hestia and Nina to scream in shock at the brutal act committed by Hedin. Stunned, Nina hesitated to help, but a golden-haired renard quickly cradled Bell’s head in her lap, while another girl began treating the burn injuries on his neck. The pallum and the elf criticized the excessive violence they had just witnessed.\n",
    "\n",
    "The atmosphere was tense, with Hestia trembling beside them, caught between rage and restraint. Confusion and frustration hung thick in the air as Bell lay unconscious, receiving care from the girls—which annoyed Loki, due to Bell’s growing popularity among them. Mikoto tried to reassure Nina with a simple gesture, implying that experience made the difference in situations like this. Meanwhile, another girl watched silently, horrified by how normalized such violence had become in Bell’s life.\n",
    "\n",
    "Hedin, seemingly unbothered by the unfolding chaos, turned to Leon and noted that he had also inhaled miasma from the Valley. Leon acknowledged this, adding that Nina’s purification magic had mostly protected them. Realizing this, Nina regained her composure.\n",
    "\n",
    "Hedin went on to criticize Bell’s overreliance on his strength, explaining that he wouldn’t wake until he had fully recovered from exhaustion. The only reason he had forcefully put Bell to sleep in the first place was because he had noticed that Bell’s stamina recovery was slower than usual—something Bell himself had failed to recognize.\n",
    "\n",
    "He emphasized that even high-level adventurers like Bell could misjudge their limits. Hestia gently lifted Bell onto her lap and applied a tranquilizing potion, the calming scent spreading through the room. She argued with Hedin, worried that if Bell fell too deeply into sleep, he might not wake up again—but he dismissed her concern.\n",
    "\n",
    "Their argument was interrupted by Leon, who urged calmness. He questioned Hedin about the rising tensions, but Hedin stressed the importance of teamwork, especially since they had already suffered casualties. This revelation shocked Nina, while Leon’s expression hardened.\n",
    "\n",
    "As tensions simmered, Hestia and Ryu noticed Syr quietly mourning the loss of [[Freya Familia]|her Familia members]. Syr revealed that three of them—Meluna, Restan, and Tanna—had perished during the expedition. They had been seasoned warriors who had fought alongside Bell and taught him valuable lessons.\n",
    "\n",
    "Welf observed the somber mood, silently agreeing that keeping Bell asleep was the right choice. He was too kind-hearted to cope with such overwhelming sorrow. A heavy silence filled the room as the reality of loss settled over everyone.\n",
    "\n",
    "Hedin, known for his strict nature, reluctantly acknowledged that the team had made the right call under the circumstances. Lili expressed disbelief at the Loki Familia’s forced retreat, highlighting the severity of the situation. Even adventurers like Finn, Riveria, Gareth, Tione, Tiona, Bete, Anakitty, Ais, Tsubaki, and Amid had not returned from the 60th floor. Their failure to escape suggested that even the strongest could be overwhelmed in the Dungeon’s deeper levels.\n",
    "\n",
    "Syr then shared critical details: Ottar was still on the 49th floor, while Hogni had survived—but neither had returned. This news shocked Lili and the others, deepening the tension in the room. Questions arose about how Syr had access to such specific information, prompting Leon to ask about the source. Loki explained that the adventurers had been given an Eye Crystal to maintain communication until it shattered during the battle. Hestia confirmed that they had received this information directly from Loki.\n",
    "\n",
    "A notable gap in knowledge had formed between those newly returned to the city and those who had already received updates. Discussions of a potential rescue mission began, with plans forming for immediate action. Mikoto, though visibly anxious, took a deep breath to steady herself. Welf presented a batch of Magic Swords imbued with elemental attributes. Hedin snatched the list from him without a word, signaling his approval of their readiness.\n",
    "\n",
    "The Hestia Familia remained resolute in their decision to fight. Haruhime appeared emotional, while Lili stood pale and silent. Ryu quietly assessed the strength of their allies. Nina, sensing the group’s resolve, asked what their next steps were, emphasizing the importance of preparation for the Dungeon’s dangers.\n",
    "\n",
    "She voiced concern over critical needs—water, food, and supplies—at such deep levels, reflecting on her own limited experience and feeling overwhelmed by the prospect of going deeper. She questioned how they would reach those trapped below, her doubt and worry surfacing. She also asked whether Lili and the others would be going, clearly worried about the number of adventurers capable of making the journey.\n",
    "\n",
    "Abruptly, Hedin intervened, declaring that he would not leave any strength unused. He looked directly at Nina, his intense presence silencing her. He stated that Orario’s entire army would be mobilized for the mission, underscoring the gravity of the task ahead.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fca2e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "บทที่ 1: SOS ของไอส์\n",
      "\n",
      "การเดินทางสำรวจของลอคิ ฟามิเลียลากลับมาด้วยความล้มเหลว และบรรยากาศเต็มไปด้วยความสิ้นหวังเมื่อรายงานยืนยันว่าพวกเขาสูญเสียอย่างรุนแรงที่ชั้นที่ 60 นักผจญภัยและผู้สังเกตการณ์ต่างตกตะลึงและหวาดกลัวเมื่อข่าวแพร่สะพัดไปทั่วเมือง ทำให้พวกเขาเสียใจและไม่เชื่อในสิ่งที่เกิดขึ้น ภาพของเพื่อนร่วมทางที่ล้มตายอยู่ เลือดเปื้อนและบาดเจ็บ ทำให้ผู้ที่เห็นภาพนั้นรู้สึกเศร้าสลด\n",
      "\n",
      "ท่ามกลางความโกลาหล ราอูลกอดแขนที่บาดเจ็บของเขาแน่น และยืนกรานว่าพวกเขาต้องช่วยเพื่อนร่วมทางที่ยังติดอยู่ในชั้นลึก ความจริงที่ว่านักผจญภัยผู้กล้าหาญไม่กลับมา รวมถึงนักรักษาและหน่วยสนับสนุน ทำให้เกิดความโกรธและความเศร้า ความโกลาหลในเมืองออราคิโอปะทุขึ้น เบลล์ตระหนักว่าเขาหาไอส์ไม่พบในกลุ่มที่กลับมา ทำให้หัวใจของเขาสั่นแรง และแม้จะรู้ถึงสถานการณ์ เขาก็ยังตะโกนชื่อเธอด้วยความสิ้นหวัง ทันทีที่เขาเตรียมจะวิ่งไปข้างหน้า ไอนาก็ยับยั้งเขาไว้ ขอให้เขาหนีไปจากความโกลาหลที่อึดอัดใจ\n",
      "\n",
      "แม้บรรยากาศจะเต็มไปด้วยความสิ้นหวังทั่วเมือง ไอนาและสมาชิกคณะนักผจญภัยคนอื่นๆ พยายามรักษาความสงบและดูแลผู้ที่กลับมา เบลล์รู้สึกท่วมท้นใจ เขาต่อสู้กับอารมณ์ของตัวเองเมื่อเห็นการต่อสู้ของไอนาเองที่เต็มไปด้วยความกลัวและความเศร้า เขาโหยหาความมั่นใจจากเพื่อนร่วมทางอย่างฟินน์และคนอื่นๆ ขณะที่ต้องเผชิญกับความจริงที่ว่าพวกเขาไม่อยู่\n",
      "\n",
      "ลีออนวางมือที่ปลอบโยนไว้บนไหล่ของเบลล์ กระตุ้นให้เขาสงบ และเน้นย้ำถึงความรับผิดชอบที่หนักอึ้ง ไอนายังคงเป็นเสมือนแสงแห่งความเข้มแข็งที่เปราะบาง เธอยังให้กำลังใจแม้จะหวาดกลัวเอง เบลล์รู้สึกเชื่อมโยงกับเธออย่างลึกซึ้ง สะท้อนถึงการขอโทษของเธอต่อสถานการณ์ที่พวกเขาเผชิญอยู่\n",
      "\n",
      "ขณะที่นักรักษาเร่งเข้าไปช่วยเหลือผู้บาดเจ็บ ภาพก็ยิ่งดูเศร้าสลดลง เสียงร้องขอความยุติธรรมและความโกรธจากก่อนหน้านี้ดังก้องอย่างเงียบงันท่ามกลางการถูกแบกผู้บาดเจ็บออกไป ในที่สุด เฮสเทียและสมาชิกฟามิเลียลของเธอได้กลับมาพบเบลล์อีกครั้ง นำความเร่งด่วนและความห่วงใยมาสู่สถานการณ์ที่กำลังเกิดขึ้น\n",
      "\n",
      "ผู้รอดชีวิตจากการ \"การเดินทางสำรวจ\" ถูกนำไปยังสถานพยาบาลพิเศษที่ดำเนินการโดยคณะนักผจญภัยไดอัน เซคท์ ในห้องพักผู้ป่วยของสมาชิกฟามิเลียลเฮสเทีย ลีออน และนีน่า—ยกเว้นไอนาที่กำลังปฏิบัติหน้าที่ของคณะนักผจญภัย ลิลลี่ดูตึงเครียด มิโคโตะก้มหน้าลง และฮารุฮิเมะพยายามปลอบโยนพวกเขา เวลฟ์ดูสงบข้างๆ ลีออน แม้ว่าเขาจะกำลังกดความกลัวไว้ เฮสเทียที่กล่าวถึง \"ชั้นที่ 60\" ทำให้ทุกคนรู้สึกไม่สบายใจ\n",
      "\n",
      "ลีออนขอให้ทุกคนสงบสติอารมณ์ แต่ข่าวจากชั้นที่ 60 ทำให้พวกเขารู้สึกหวาดระแวง สัตว์ประหลาดตัวหนึ่งปรากฏตัวขึ้น และลอคิ เทพธิดาแห่งคณะนักผจญภัยลอคิมาถึงด้วยสีหน้าวิตกกังวล เธอยืนยันว่าบอสดันเจี้ยนหายไปแล้ว และ \"ชั้นลึก\" ได้พังถล่มลงมา ทำให้สัตว์ประหลาดที่ทรงพลังกำลังเร่ร่อนอยู่เบื้องล่าง สีหน้าของลอคิแสดงทั้งความโกรธและความเจ็บปวด ขณะที่เธอยืนยันว่าลูกๆ ของเธอยังติดอยู่ในดันเจี้ยน—และพวกเขาต้องได้รับการช่วยเหลือทันที\n",
      "\n",
      "หัวใจของเบลล์เต้นแรงด้วยความกลัวและความหวังเมื่อเขาตระหนักว่าไอส์และคนอื่นๆ ยังมีชีวิตอยู่ เพราะพรที่พวกเขาได้รับยังไม่ลดลง เทพเจ้าอื่นๆ สามารถรับรู้ถึงลูกๆ ของพวกเขาผ่านสายเลือดศักดิ์สิทธิ์ที่เชื่อมโยงกับสายเลือดของพวกเขานั้น ซึ่งมอบพรเหล่านั้นให้พวกเขา ความจริงนี้จุดประกายความมุ่งมั่นของเบลล์ที่จะช่วย และเขาวิงวอนให้ลอคิปล่อยให้เขาเข้าร่วมทีมกู้ภัย\n",
      "\n",
      "ลอคิดูขุ่นเคืองกับความกระตือรือร้นของเขา—แม้ว่าเธอจะมาหาพวกเขาเพื่อขอความช่วยเหลือ ก็มีช่วงเวลาแห่งความสนุกสนานเมื่อเฮสเทียและซายร์แกล้งลอคิอย่างเล่นๆ ทำให้บรรยากาศในห้องผ่อนคลายลง ลีออนวางมือปลอบโยนไว้บนไหล่ของเบลล์ พร้อมให้กำลังใจท่ามกลางความจริงจังของสถานการณ์ จากนั้นเฮดินก็แสดงตัวออกมา ประกาศว่าเขาจะเป็นผู้บัญชาการในการปฏิบัติการกู้ภัย และทุกคนจะอยู่ภายใต้คำสั่งของเขา\n",
      "\n",
      "เขามีประสบการณ์ที่จำเป็นในการจัดการภารกิจขนาดใหญ่ เนื่องจากบทบาทผู้นำของเขาในสงครามคณะนักผจญภัย แม้บรรยากาศจะหนักอึ้ง เบลล์ก็รู้สึกเป็นเกียรติที่ได้เป็นหนึ่งในนักผจญภัยชั้นแนวหน้าที่ถูกเรียกตัวเข้าปฏิบัติภารกิจ เขาถามเฮดินว่าเขาจะทำอะไรเพื่อช่วย แต่เขากลับประหลาดใจเมื่อเฮดินบอกให้เขานอนหลับ เบลล์สับสน—เขาจะพักได้ยังไงในเมื่อสถานการณ์เร่งด่วนขนาดนี้?\n",
      "\n",
      "แม้เบลล์จะคัดค้าน เฮดินก็ยืนยันว่าเบลล์ต้องฟื้นฟูจากความเหนื่อยล้าที่สะสมมาจากการทดสอบล่าสุดในหุบเขา จากนั้นเฮดินใช้เวทมนตร์บังคับให้เบลล์หลับไป เบลล์หมดสติหลังจากถูกเวทมนตร์ของเฮดินโจมตี\n",
      "\n",
      "เบลล์ล้มลงบนพื้น ทำให้เฮสเทียและนีน่ากรีดร้องด้วยความตกใจต่อการกระทำอันโหดร้ายของเฮดิน นีน่าตกตะลึงและลังเลที่จะช่วย แต่เรนาร์ดผมทองก็โอบกอดศีรษะของเบลล์ไว้ในอ้อมแขน ขณะที่เด็กสาวอีกคนเริ่มรักษาแผลไฟไหม้ที่คอของเขา พัลลัมและเอลฟ์วิจารณ์ความรุนแรงที่พวกเขาเพิ่งเห็น\n",
      "\n",
      "บรรยากาศเต็มไปด้วยความตึงเครียด เฮสเทียตัวสั่นอยู่ข้างๆ พวกเขา สับสนและหงุดหงิดเต็มไปหมดขณะที่เบลล์หมดสติและกำลังได้รับการดูแลจากเด็กสาว—ซึ่งทำให้ลอคิไม่พอใจ เนื่องจากความนิยมของเบลล์ที่เพิ่มขึ้นในหมู่พวกเขา มิโคโตะพยายามปลอบโยนนีน่าด้วยท่าทางง่ายๆ สื่อว่าประสบการณ์ต่างหากที่ทำให้สถานการณ์เช่นนี้แตกต่างไปได้ ในขณะเดียวกัน เด็กสาวอีกคนเฝ้ามองอย่างเงียบๆ ด้วยความหวาดกลัวต่อความรุนแรงที่กลายเป็นเรื่องปกติในชีวิตของเบลล์\n",
      "\n",
      "เฮดิน ดูเหมือนจะไม่ใส่ใจกับความโกลาหลที่เกิดขึ้น เขาหันไปหาลีออนและกล่าวว่าเขาก็สูดดมกลิ่นพิษจากหุบเขาเช่นกัน ลีออนยอมรับและกล่าวเพิ่มเติมว่าเวทมนตร์ชำระล้างของนีน่าช่วยปกป้องพวกเขาได้เป็นส่วนใหญ่ เมื่อตระหนักถึงเรื่องนี้ นีน่าก็กลับมามีสติอีกครั้ง\n",
      "\n",
      "เฮดินวิจารณ์การพึ่งพาความแข็งแกร่งของเบลล์มากเกินไป โดยอธิบายว่าเขาจะยังไม่ตื่นจนกว่าจะฟื้นตัวจากความเหนื่อยล้าอย่างเต็มที่ เหตุผลเดียวที่เขาบังคับให้เบลล์หลับไปในตอนแรกก็เพราะเขาสังเกตเห็นว่าการฟื้นฟูพละกำลังของเบลล์ช้ากว่าปกติ—ซึ่งเบลล์เองก็ไม่เคยตระหนักถึง\n",
      "\n",
      "เขาเน้นย้ำว่าแม้แต่นักผจญภัยระดับสูงอย่างเบลล์ก็อาจประเมินขีดจำกัดของตนเองผิดพลาดได้ เฮสเทียค่อยๆ ยกเบลล์ขึ้นมาบนตักและหยอดยานอนหลับลงไป กลิ่นกล่อมกล่อมแพร่กระจายไปทั่วห้อง เธอโต้เถียงกับเฮดินด้วยความกังวลว่าหากเบลล์หลับลึกเกินไป เขาอาจไม่ตื่นขึ้นมาอีก—แต่เขาก็ไม่สนใจความกังวลของเธอ\n",
      "\n",
      "การโต้เถียงของพวกเขาถูกขัดจังหวะโดยลีออนที่ขอให้ทุกคนสงบสติอารมณ์ เขาตั้งคำถามกับเฮดินเกี่ยวกับความตึงเครียดที่เพิ่มขึ้น แต่เฮดินเน้นย้ำถึงความสำคัญของทีมงาน โดยเฉพาะอย่างยิ่งเมื่อพวกเขาสูญเสียผู้คนไปแล้ว การเปิดเผยนี้ทำให้นีน่าตกใจ ขณะที่สีหน้าของลีออนแข็งกร้าวขึ้น\n",
      "\n",
      "ขณะที่ความตึงเครียดเพิ่มขึ้น เฮสเทียและไรวุจสังเกตเห็นซายร์กำลังคร่ำครวญอย่างเงียบๆ ต่อการสูญเสียสมาชิกในคณะของเธอ ไรวุจเปิดเผยว่ามีสามคน—เมลูน่า เรสตัน และแทนนา—เสียชีวิตระหว่างการเดินทางสำรวจ พวกเขาเป็นนักรบที่ผ่านประสบการณ์และเคยร่วมรบเคียงข้างเบลล์และสอนบทเรียนอันมีค่าให้เขา\n",
      "\n",
      "เวลฟ์สังเกตเห็นบรรยากาศที่เศร้าสลด และเห็นด้วยอย่างเงียบๆ ว่าการหลับเบลล์เป็นทางเลือกที่ถูกต้อง เขาเป็นคนใจดีเกินกว่าจะรับมือกับความเศร้าโศกที่รุนแรงเช่นนี้ได้ ความเงียบอันหนักอึ้งเต็มไปทั่วห้องขณะที่ความจริงของการสูญเสียเริ่มฝังลึกในใจของทุกคน\n",
      "\n",
      "เฮดิน ซึ่งเป็นที่รู้จักในเรื่องความเข้มงวด ยอมรับอย่างไม่เต็มใจว่าทีมงานตัดสินใจถูกต้องภายใต้สถานการณ์นี้ ลิลลี่แสดงความไม่เชื่อในการถอยกลับของคณะนักผจญภัยลอคิ โดยเน้นย้ำถึงความรุนแรงของสถานการณ์ แม้แต่นักผจญภัยอย่างฟินน์ รีเวเรีย แกเรธ ไทโอน ไทโอนา เบเต อานากิตตี ไอส์ ซูบากิ และอามิดก็ยังไม่กลับมาจากชั้นที่ 60 ความล้มเหลวในการหนีรอดแสดงให้เห็นว่าแม้แต่นักผจญภัยที่แข็งแกร่งที่สุดก็อาจถูกความดันในดันเจี้ยนระดับลึกซึ้งได้\n",
      "\n",
      "ซายร์จากไปก็เล่ารายละเอียดสำคัญว่า ออตตาร์ยังอยู่ที่ชั้นที่ 49 ขณะที่โฮนีรอดชีวิต—แต่ทั้งคู่ยังไม่กลับมา ข่าวนี้ทำให้ลิลลี่และคนอื่นๆ ตกใจยิ่งขึ้น ทำให้บรรยากาศในห้องตึงเครียดขึ้นอีก การตั้งคำถามเกี่ยวกับที่มาของข้อมูลที่ซายร์เข้าถึง ทำให้ลีออนถามถึงแหล่งที่มา ลอคิอธิบายว่านักผจญภัยได้รับคริสตัลตาเพื่อรักษาการสื่อสารไว้จนกระทั่งมันแตกหักระหว่างการต่อสู้ เฮสเทียยืนยันว่าพวกเขาได้รับข้อมูลนี้โดยตรงจากลอคิ\n",
      "\n",
      "ช่องว่างความรู้ที่สำคัญเกิดขึ้นระหว่างผู้ที่เพิ่งกลับมายังเมืองกับผู้ที่ได้รับข้อมูลมาแล้ว การพูดคุยเกี่ยวกับการเดินทางสำรวจที่อาจเกิดขึ้นเริ่มขึ้น พร้อมกับแผนการที่จะดำเนินการทันที มิโคโตะ แม้จะดูวิตกกังวลอย่างเห็นได้ชัด แต่ก็หายใจลึกๆ เพื่อให้ตัวเองสงบลง เวลฟ์นำชุดดาบเวทมนตร์ที่บรรจุธาตุต่างๆ มาให้ เบดินคว้ารายการนั้นไปโดยไม่พูดอะไร เป็นสัญญาณว่าเขาเห็นด้วยกับความพร้อมของพวกเขา\n",
      "\n",
      "คณะนักผจญภัยเฮสเทียยังคงมุ่งมั่นในการตัดสินใจที่จะต่อสู้ ฮารุฮิเมะดูมีอารมณ์ ขณะที่ลิลลี่ยืนนิ่งและเงียบ ไรวุจประเมินความแข็งแกร่งของพันธมิตรอย่างเงียบๆ นีน่าซึ่งรับรู้ถึงความมุ่งมั่นของกลุ่ม ก็ถามว่าขั้นตอนต่อไปคืออะไร โดยเน้นย้ำถึงความสำคัญของการเตรียมตัวสำหรับอันตรายในดันเจี้ยน\n",
      "\n",
      "เธอกังวลเกี่ยวกับความต้องการที่สำคัญ—น้ำ อาหาร และเสบียง—ในระดับลึกเช่นนี้ สะท้อนถึงประสบการณ์ที่จำกัดของเธอเอง และรู้สึกท่วมท้นใจกับความคิดที่จะต้องลงไปลึกกว่านี้ เธอตั้งคำถามว่าพวกเขาจะไปถึงผู้ที่ติดอยู่เบื้องล่างได้อย่างไร ความสงสัยและความกังวลของเธอปรากฏออกมา เธอยังถามด้วยว่าลิลลี่และคนอื่นๆ จะไปหรือไม่ โดยกังวลอย่างชัดเจนเกี่ยวกับจำนวนนักผจญภัยที่สามารถเดินทางไปได้\n",
      "\n",
      "ทันใดนั้น เฮดินก็เข้ามาแทรกขึ้นมา ประกาศว่าเขาจะไม่ปล่อยให้ความแข็งแกร่งใดๆ ถูกปล่อยเปล่า เขาหันไปมองนีน่าโดยตรง การปรากฏตัวที่เข้มข้นของเขาทำให้เธอเงียบลง เขากล่าวว่ากองทัพทั้งหมดของออราคิโอจะถูกระดมพลสำหรับภารกิจนี้ ย้ำถึงความร้ายแรงของภารกิจที่รออยู่ข้างหน้า\n"
     ]
    }
   ],
   "source": [
    "# ข้อความที่ต้องการแปล\n",
    "text5 = LN_part2\n",
    "\n",
    "# สำคัญ: ต้องใช้ System Prompt ตามที่โมเดลกำหนดเพื่อให้ผลลัพธ์แม่นยำ\n",
    "messages5 = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following text into Thai.\"},\n",
    "    {\"role\": \"user\", \"content\": text5},\n",
    "]\n",
    "\n",
    "# เตรียม Input\n",
    "input_ids5 = tokenizer.apply_chat_template(\n",
    "    messages5, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# สั่งให้โมเดล Generate ผลลัพธ์\n",
    "outputs5 = model.generate(\n",
    "    input_ids5, \n",
    "    max_new_tokens=8000, \n",
    "    do_sample=False # แนะนำให้ปิด sampling เพื่อความแม่นยำในการแปล\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs5[0][len(input_ids5[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc345de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 9523 characters.\n",
      "Splitting text into chunks with max 200 tokens each...\n",
      "Text split into 12 chunk(s).\n",
      "\n",
      "Translating from English to Thai...\n",
      "Translation completed.\n",
      "\n",
      "--- Chunk Breakdown ---\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Original: Chapter 1: Ais SOS Loki Familia's expedition has failed, and despair filled the air as reports confi...\n",
      "Translated: บทที่ 1: SOS ของไอส์ โลคิ และคณะสำรวจของฟามิเลียลากลับมาไม่สำเร็จ อากาศเต็มไปด้วยความสิ้นหวังเมื่อรา...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Original: realized that he couldn't find Ais among the returned, causing his heart to race, and despite knowin...\n",
      "Translated: เขาตระหนักได้ว่าเขาหาไอส์ไม่พบในกลุ่มผู้ที่กลับมา ทำให้หัวใจของเขาสั่นแรง และแม้จะรู้ถึงสถานการณ์ที่...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Original: a deep connection with her, echoing her apologies for the situation they faced. As healers rushed in...\n",
      "Translated: ความผูกพันลึกซึ้งกับเธอ สะท้อนเสียงขอโทษของเธอต่อสถานการณ์ที่พวกเขาเผชิญอยู่ ขณะที่เหล่านักบำบัดเร่ง...\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Original: the \"60th floor\" made everyone uneasy. Leon urged everyone to stay calm, but news from the 60th floo...\n",
      "Translated: ชั้นที่ 60 ทำให้ทุกคนรู้สึกไม่สบายใจ ลีออนพยายามให้ทุกคนใจเย็น แต่ข่าวจากชั้นที่ 60 กลับทำให้พวกเขาร...\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Original: urgently begged Loki to let him join the rescue team. Loki seemed exasperated by his eagerness—thoug...\n",
      "Translated: เขาอ้อนวอนอย่างเร่งร้อนให้โลกิยอมให้เขาเข้าร่วมทีมกู้ภัย โลกิดูเหมือนจะหงุดหงิดกับความกระตือรือร้นขอ...\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Original: rest when the situation was so urgent? Despite his protests, Hedin insisted that Bell recover from t...\n",
      "Translated: พักเมื่อสถานการณ์เร่งด่วนขนาดนั้นหรือ? แม้เบลล์จะคัดค้าน เฮดินยืนกรานให้เบลล์ฟื้นตัวจากความเหนื่อยล้...\n",
      "\n",
      "--- Chunk 7 ---\n",
      "Original: Loki, due to Bell’s growing popularity among them. Mikoto tried to reassure Nina with a simple gestu...\n",
      "Translated: โลคิเริ่มแสดงท่าทีสนใจมากขึ้น เนื่องจากความนิยมของเบลล์ที่เพิ่มขึ้นในหมู่พวกเขา มิโคโตพยายามปลอบโยนน...\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Original: recognize. He emphasized that even high-level adventurers like Bell could misjudge their limits. Hes...\n",
      "Translated: ตระหนักได้ เขาเน้นย้ำว่าแม้แต่ผู้ผจญภัยระดับสูงอย่างเบลล์ก็อาจประเมินขีดจำกัดของตนเองผิดพลาดได้ เฮสเ...\n",
      "\n",
      "--- Chunk 9 ---\n",
      "Original: perished during the expedition. They had been seasoned warriors who had fought alongside Bell and ta...\n",
      "Translated: พวกเขาเสียชีวิตระหว่างการเดินทาง พวกเขาเป็นนักรบที่ผ่านการต่อสู้มาอย่างหนักและเคยร่วมรบเคียงข้างเบลล...\n",
      "\n",
      "--- Chunk 10 ---\n",
      "Original: deeper levels. Syr then shared critical details: Ottar was still on the 49th floor, while Hogni had ...\n",
      "Translated: ระดับที่ลึกขึ้น ซีร์จึงแบ่งปันรายละเอียดสำคัญว่า ออตตาร์ยังคงอยู่ที่ชั้นที่ 49 ขณะที่โฮญีรอดชีวิต—แต...\n",
      "\n",
      "--- Chunk 11 ---\n",
      "Original: word, signaling his approval of their readiness. The Hestia Familia remained resolute in their decis...\n",
      "Translated: คำพูดของเขาแสดงถึงการอนุมัติว่าพวกเขาพร้อมแล้ว ครอบครัวเฮสเธียยังคงมุ่งมั่นในการตัดสินใจที่จะต่อสู้ ...\n",
      "\n",
      "--- Chunk 12 ---\n",
      "Original: Nina, his intense presence silencing her. He stated that Orario’s entire army would be mobilized for...\n",
      "Translated: นีน่ารู้สึกอึ้งไปเลยเมื่อได้สัมผัสถึงความเข้มข้นของเขาที่ทำให้เธอเงียบสนิท เขาประกาศว่ากองทัพทั้งหมด...\n",
      "\n",
      "----------------------\n",
      "\n",
      "Final combined translation (Thai):\n",
      "บทที่ 1: SOS ของไอส์ โลคิ และคณะสำรวจของฟามิเลียลากลับมาไม่สำเร็จ อากาศเต็มไปด้วยความสิ้นหวังเมื่อรายงานยืนยันความสูญเสียอย่างรุนแรงบนชั้นที่ 60 นักผจญภัยและผู้สังเกตการณ์ต่างตกตะลึงและหวาดกลัวเมื่อข่าวแพร่สะพัดไปทั่วเมือง ทำให้พวกเขาต้องเผชิญกับความเศร้าโศกและความไม่เชื่อ ภาพของเพื่อนร่วมทางที่ล้มตายและได้รับบาดเจ็บทำให้ผู้ที่เห็นภาพนั้นรู้สึกเศร้าสลด ท่ามกลางความโกลาหล ราอูลกอดแขนที่ได้รับบาดเจ็บของเขาและยืนยันว่าพวกเขาต้องช่วยเหลือเพื่อนร่วมทางที่ยังติดอยู่ในชั้นลึก ความจริงที่นักผจญภัยผู้กล้าหาญไม่กลับมา รวมถึงนักรักษาและหน่วยสนับสนุน ทำให้เกิดความโกรธและความเศร้าใจขึ้นในหมู่ผู้คน ฝูงชนต่างส่งเสียงร้องด้วยความโกรธและความเศร้าใจ และหลายคนไม่อาจรับความจริงของสถานการณ์ได้อีกต่อไป จนต้องทรุดลงภายใต้น้ำหนักของความสิ้นหวัง ขณะที่ความโกลาหลปะทุขึ้นในออราริโอ เบลล์เขาตระหนักได้ว่าเขาหาไอส์ไม่พบในกลุ่มผู้ที่กลับมา ทำให้หัวใจของเขาสั่นแรง และแม้จะรู้ถึงสถานการณ์ที่เกิดขึ้น เขาก็ยังตะโกนเรียกชื่อเธอด้วยความสิ้นหวัง ขณะที่เขากำลังจะวิ่งเข้าไปข้างหน้า ไอนาได้หยุดเขาไว้ พร้อมกับกระตุ้นให้เขาหนีออกไปจากความวุ่นวายที่กำลังบีบคั้น แม้ว่าเมืองจะเต็มไปด้วยความสิ้นหวัง แต่ไอนาและสมาชิกคณะกรรมาธิการคนอื่นๆ ก็พยายามรักษาความสงบและดูแลผู้ที่กลับมา เบลล์รู้สึกอัดอั้นตันในใจ เขาต่อสู้กับอารมณ์ของตัวเองขณะที่ได้เห็นการต่อสู้ของไอนาเองกับความกลัวและความเศร้า เขาโหยหาความมั่นใจจากเพื่อนร่วมทางอย่างฟินน์และคนอื่นๆ ขณะที่ต้องเผชิญกับความจริงที่ว่าพวกเขาไม่อยู่แล้ว ลีออนวางมือที่อบอุ่นไว้บนไหล่ของเบลล์ พร้อมกับกระตุ้นให้เขาสงบสติ และเน้นย้ำถึงความสำคัญของหน้าที่ที่เขากำลังแบกรับ ไอนายังคงเป็นเสมือนแสงสว่างแห่งความเข้มแข็งที่เปราะบาง เธอยังคงให้กำลังใจแม้จะหวาดกลัวเอง เบลล์รู้สึกความผูกพันลึกซึ้งกับเธอ สะท้อนเสียงขอโทษของเธอต่อสถานการณ์ที่พวกเขาเผชิญอยู่ ขณะที่เหล่านักบำบัดเร่งเข้าไปช่วยเหลือผู้บาดเจ็บ ภาพที่ปรากฏก็ยิ่งดูหม่นหมองขึ้น เสียงร้องขอความยุติธรรมและความโกรธแค้นที่เคยดังขึ้นก่อนหน้านี้กลับดังก้องอย่างเงียบงัน ท่ามกลางภาพผู้บาดเจ็บถูกอุ้มออกมา ในที่สุด เฮสเธียและสมาชิกที่เหลือของเฮสเธีย ฟามิเลีย ก็กลับมาพบเบลล์อีกครั้ง พร้อมกับความรู้สึกเร่งด่วนและความห่วงใย ท่ามกลางหายนะที่กำลังเกิดขึ้น ผู้รอดชีวิตจาก \"ภารกิจสำรวจ\" ถูกนำตัวไปยังสถานบำบัดพิเศษที่ดำเนินการโดยฟามิเลียเดียนเชคท์ ในห้องพักผู้ป่วยของฟามิเลียเฮสเธีย มีสมาชิกอย่างลีออนและนีน่า รวมถึงอีนาที่ออกปฏิบัติหน้าที่ของสมาคม ลิลลี่ดูตึงเครียด มิโคโตะก้มหน้าลง ส่วนฮารุฮิมะพยายามปลอบโยนพวกเขา เวลฟ์ดูสงบนิ่งข้างๆ ลีออน แม้ว่าเขาอาจกำลังเก็บความกลัวไว้ เฮสเธียกล่าวถึงชั้นที่ 60 ทำให้ทุกคนรู้สึกไม่สบายใจ ลีออนพยายามให้ทุกคนใจเย็น แต่ข่าวจากชั้นที่ 60 กลับทำให้พวกเขารู้สึกหวาดระแวง ปีศาจตัวหนึ่งได้ปรากฏตัวขึ้น และเทพธิดาล็อกกี้แห่งลอคิฟาเมเลียก็มาถึงด้วยท่าทีวิตกกังวล เธอยืนยันว่าบอสดันเจี้ยนได้จากไปแล้ว และชั้นลึกได้พังถล่มลงมา ทำให้ปีศาจที่ทรงพลังตัวหนึ่งกำลังเร่ร่อนอยู่เบื้องล่าง สีหน้าของลอคตี้แสดงทั้งความโกรธและความเจ็บปวด ขณะที่เธอยืนยันว่าลูกๆ ของเธอยังคงติดอยู่ในดันเจี้ยน—และพวกเขาต้องได้รับการช่วยเหลือทันที หัวใจของเบลล์เต้นแรงด้วยความกลัวและความหวังเมื่อเขาตระหนักว่าไอส์และคนอื่นๆ ยังมีชีวิตอยู่ เพราะพรที่พวกเขาได้รับยังไม่ลดลง เทพอื่นๆ สามารถรับรู้ถึงลูกๆ ของพวกเขาผ่านสายเลือดศักดิ์สิทธิ์ที่เชื่อมโยงกับฟัลนา ซึ่งมอบพรเหล่านั้นให้พวกเขา ความจริงที่เกิดขึ้นนี้จุดประกายความมุ่งมั่นของเบลล์ที่จะช่วยเหลือ และเขาเขาอ้อนวอนอย่างเร่งร้อนให้โลกิยอมให้เขาเข้าร่วมทีมกู้ภัย โลกิดูเหมือนจะหงุดหงิดกับความกระตือรือร้นของเขา—แม้ว่าเธอจะมาหาพวกเขาเพื่อขอความช่วยเหลือก็ตาม มีช่วงเวลาเบาสบายเมื่อเฮสเธียและซียร์แกล้งล้อโลกิอย่างเล่นๆ ช่วยคลายความตึงเครียดในห้อง เลออนวางมืออันอบอุ่นบนไหล่ของเบลล์ พร้อมให้กำลังใจท่ามกลางสถานการณ์ที่จริงจัง จากนั้นเฮดินก็แสดงตัวออกมา ประกาศว่าเขาจะเป็นผู้บัญชาการภารกิจกู้ภัย และทุกคนจะอยู่ภายใต้การบังคับบัญชาของเขา เขามีประสบการณ์ที่จำเป็นในการจัดการภารกิจขนาดใหญ่ จากบทบาทผู้นำของเขาในสงครามฟาเมเลีย แม้บรรยากาศจะหนักอึ้ง แต่เบลล์ก็รู้สึกเป็นเกียรติที่ได้เป็นหนึ่งในนักผจญภัยชั้นยอดที่ถูกเรียกตัวให้เข้าร่วมภารกิจนี้ เขาถามเฮดินว่าเขาจะทำอะไรได้บ้างเพื่อช่วย แต่กลับทำให้เขาประหลาดใจที่เฮดินบอกให้เขานอนหลับ เบลล์สับสน—เขาจะทำอย่างไรได้พักเมื่อสถานการณ์เร่งด่วนขนาดนั้นหรือ? แม้เบลล์จะคัดค้าน เฮดินยืนกรานให้เบลล์ฟื้นตัวจากความเหนื่อยล้าที่สะสมมาจากการทดลองล่าสุดในหุบเขา จากนั้นเฮดินใช้เวทมนตร์บังคับให้เบลล์หลับไปอย่างรุนแรง เบลล์หมดสติหลังจากถูกโจมตีด้วยเวทมนตร์ของเฮดิน เขาร่วงลงบนพื้น ทำให้เฮสเธียและนีน่ากรีดร้องด้วยความตกใจต่อการกระทำอันโหดร้ายของเฮดิน นีน่าตกตะลึงจนลังเลที่จะช่วยเหลือ แต่เรนาร์ดผมสีทองรีบโอบศีรษะเบลล์ไว้ในตัก ขณะที่อีกเด็กผู้หญิงคนหนึ่งเริ่มรักษาแผลไฟไหม้ที่คอของเขา พัลลัมและเอลฟ์วิจารณ์ความรุนแรงที่เกินเหตุที่พวกเขาเพิ่งเห็น บรรยากาศเต็มไปด้วยความตึงเครียด โดยเฮสเธียตัวสั่นอยู่ข้างๆ พวกเขา สับสนและหงุดหงิดลอยอยู่ในอากาศขณะที่เบลล์หมดสติและกำลังได้รับการดูแลจากเด็กผู้หญิงสองคน ซึ่งทำให้พวกเขาไม่พอใจโลคิเริ่มแสดงท่าทีสนใจมากขึ้น เนื่องจากความนิยมของเบลล์ที่เพิ่มขึ้นในหมู่พวกเขา มิโคโตพยายามปลอบโยนนีน่าด้วยท่าทางง่ายๆ โดยสื่อว่าประสบการณ์ต่างหากที่ทำให้เกิดความแตกต่างในสถานการณ์เช่นนี้ ในขณะเดียวกัน เด็กสาวอีกคนหนึ่งเฝ้ามองอย่างเงียบงันด้วยความหวาดกลัวต่อความรุนแรงที่กลายเป็นเรื่องปกติในชีวิตของเบลล์ เฮดินดูเหมือนจะไม่รู้สึกกังวลกับความวุ่นวายที่เกิดขึ้น เขาหันไปพูดกับลีออนว่าลีออนเองก็สูดกลิ่นพิษจากหุบเขาเช่นกัน ลีออนยอมรับเรื่องนี้ พร้อมเสริมว่าคาถาชำระล้างของนีน่าช่วยปกป้องพวกเขาไว้ได้เป็นส่วนใหญ่ เมื่อตระหนักถึงเรื่องนี้ นีน่าจึงกลับมามีสติอีกครั้ง เฮดินยังวิจารณ์เบลล์ที่พึ่งพาศีลธรรมของเขามากเกินไป โดยอธิบายว่าเขาจะไม่ตื่นจนกว่าจะฟื้นตัวจากความเหนื่อยล้าอย่างเต็มที่ เหตุผลเดียวที่เขาบังคับให้นอนเบลล์ในตอนแรกก็เพราะสังเกตเห็นว่าการฟื้นฟูพลังของเบลล์ช้ากว่าปกติ ซึ่งเป็นเรื่องที่เบลล์เองก็ไม่สามารถทำได้ตระหนักได้ เขาเน้นย้ำว่าแม้แต่ผู้ผจญภัยระดับสูงอย่างเบลล์ก็อาจประเมินขีดจำกัดของตนเองผิดพลาดได้ เฮสเธียค่อยๆ อุ้มเบลล์ขึ้นมาวางบนตักของเธอ แล้วหยอดยาให้เขาสงบลงไป กลิ่นหอมอ่อนๆ ของยาแพร่กระจายไปทั่วห้อง เธอโต้เถียงกับเฮดินด้วยความกังวลว่าหากเบลล์หลับลึกเกินไป เขาอาจไม่ตื่นขึ้นมาอีก—แต่เขาปฏิเสธความกังวลของเธอ การโต้เถียงของพวกเขาถูกขัดจังหวะโดยเลอองที่เร่งให้ทุกคนสงบสติอารมณ์ เขาถามเฮดินถึงความตึงเครียดที่เพิ่มขึ้น แต่เฮดินเน้นย้ำถึงความสำคัญของการทำงานเป็นทีม โดยเฉพาะอย่างยิ่งเมื่อพวกเขาได้สูญเสียผู้ร่วมทางไปแล้ว การเปิดเผยนี้ทำให้นีน่าตกใจ ขณะที่สีหน้าของเลอองแข็งกร้าวขึ้น เมื่อความตึงเครียดเริ่มเดือดพล่าน เฮสเธียและริวสังเกตเห็นซียร์กำลังคร่ำครวญอย่างเงียบๆ ต่อการสูญเสียสมาชิกในแฟมิเลียของเธอ ซียร์เปิดเผยว่าสมาชิกสามคน—เมลูน่า, เรสตัน และแทนนา—ได้พวกเขาเสียชีวิตระหว่างการเดินทาง พวกเขาเป็นนักรบที่ผ่านการต่อสู้มาอย่างหนักและเคยร่วมรบเคียงข้างเบลล์ พร้อมสอนบทเรียนอันมีค่าให้เขา เวลฟ์สังเกตเห็นบรรยากาศอันเศร้าหมองและเห็นด้วยอย่างเงียบๆ ว่าการทำให้เบลล์หลับใหลนั้นเป็นทางเลือกที่ถูกต้อง เขาเป็นคนจิตใจอ่อนโยนเกินกว่าจะรับมือกับความโศกเศร้าอันรุนแรงเช่นนี้ ห้องเงียบสนิทด้วยความโศกเศร้าเมื่อทุกคนตระหนักถึงความสูญเสีย เฮดิน ผู้เป็นที่รู้จักในความเข้มงวดของเขา ยอมรับอย่างไม่เต็มใจว่าทีมได้ตัดสินใจถูกต้องภายใต้สถานการณ์นั้น ลิลี่แสดงความไม่เชื่อกับการถอยทัพที่ถูกบังคับของลอคิฟาเมเลีย ซึ่งเน้นย้ำถึงความรุนแรงของสถานการณ์ แม้แต่นักผจญภัยอย่างฟินน์, ริเวเรีย, แกเรธ, ไทโอน, ไทโอนา, เบเต, อนาคิตตี, ไอส์, ซูบากิ และอามิด ก็ยังไม่กลับมาจากชั้นที่ 60 ความล้มเหลวในการหนีของพวกเขาแสดงให้เห็นว่าแม้แต่ผู้ที่แข็งแกร่งที่สุดก็อาจถูกท่วมท้นในดันเจี้ยนนี้ได้ระดับที่ลึกขึ้น ซีร์จึงแบ่งปันรายละเอียดสำคัญว่า ออตตาร์ยังคงอยู่ที่ชั้นที่ 49 ขณะที่โฮญีรอดชีวิต—แต่ทั้งคู่ยังไม่กลับมา ข่าวนี้ทำให้ลิลี่และคนอื่นๆ ตกใจและเพิ่มความตึงเครียดในห้อง คำถามเกิดขึ้นเกี่ยวกับวิธีที่ซีร์ได้ข้อมูลที่เฉพาะเจาะจงเช่นนี้ ทำให้ลีออนถามถึงแหล่งที่มา โลกิอธิบายว่านักผจญภัยได้รับอายคริสตัลเพื่อรักษาการติดต่อสื่อสารจนกระทั่งมันแตกสลายระหว่างการต่อสู้ เฮสเธียยืนยันว่าพวกเขาได้รับข้อมูลนี้โดยตรงจากโลกิ ช่องว่างที่น่าสังเกตในความรู้เกิดขึ้นระหว่างผู้ที่เพิ่งกลับมายังเมืองกับผู้ที่ได้รับข้อมูลไปแล้ว การพูดคุยเกี่ยวกับภารกิจช่วยเหลือที่อาจเกิดขึ้นเริ่มต้นขึ้น พร้อมวางแผนสำหรับการดำเนินการทันที มิโคโตะ แม้จะดูวิตกกังวลอย่างเห็นได้ชัด แต่ก็หายใจลึกๆ เพื่อให้ตัวเองสงบลง เวลฟ์นำชุดดาบเวทมนตร์ที่อัดแน่นไปด้วยธาตุต่างๆ มาให้ เฮดินคว้ารายการนั้นจากเขาโดยไม่ทันได้พูดอะไรคำพูดของเขาแสดงถึงการอนุมัติว่าพวกเขาพร้อมแล้ว ครอบครัวเฮสเธียยังคงมุ่งมั่นในการตัดสินใจที่จะต่อสู้ ฮารุฮิเมะดูมีอารมณ์ร่วม ขณะที่ลิลลี่ยืนนิ่งเงียบด้วยสีหน้าซีดเซียว เรียวค่อยๆ ประเมินความแข็งแกร่งของพันธมิตร นีน่าซึ่งรู้สึกถึงความมุ่งมั่นของกลุ่ม จึงถามถึงขั้นตอนต่อไป พร้อมเน้นย้ำถึงความสำคัญของการเตรียมพร้อมสำหรับอันตรายในดันเจี้ยน เธอแสดงความกังวลเกี่ยวกับสิ่งจำเป็นสำคัญ เช่น น้ำ อาหาร และเสบียง ในระดับที่ลึกเช่นนี้ สะท้อนถึงประสบการณ์ที่จำกัดของเธอเอง และรู้สึกท่วมท้นกับความคิดที่จะต้องเดินทางลึกลงไปอีก เธอตั้งคำถามว่าพวกเขาจะไปช่วยเหลือผู้ที่ติดอยู่ด้านล่างได้อย่างไร ความสงสัยและความกังวลของเธอก็ปรากฏออกมา เธอยังถามด้วยว่าลิลลี่และคนอื่นๆ จะไปหรือไม่ โดยแสดงความกังวลอย่างชัดเจนเกี่ยวกับจำนวนนักผจญภัยที่มีความสามารถในการเดินทางครั้งนี้ ทันใดนั้น เฮดินก็เข้ามาแทรกขึ้นมาประกาศว่าเขาจะไม่ปล่อยให้ความแข็งแกร่งใดๆ ถูกปล่อยเปล่า เขาหันไปมองตรงที่นีน่ารู้สึกอึ้งไปเลยเมื่อได้สัมผัสถึงความเข้มข้นของเขาที่ทำให้เธอเงียบสนิท เขาประกาศว่ากองทัพทั้งหมดของออราริโอจะถูกเรียกตัวมาปฏิบัติภารกิจนี้ ซึ่งยิ่งตอกย้ำถึงความร้ายแรงของภารกิจที่รออยู่ข้างหน้า\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # --- 1. โหลด Tokenizer และ Model ---\n",
    "# MODEL_ID = \"scb10x/typhoon-translate-4b\"\n",
    "# print(f\"Loading model '{MODEL_ID}'...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "\n",
    "# --- 2. ฟังก์ชันแบ่งข้อความตาม token จริงของโมเดล ---\n",
    "def chunk_text_by_token(text: str, max_tokens_per_chunk: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    แบ่งข้อความเป็นชิ้นๆ โดยนับ token ด้วย tokenizer ของ Typhoon เอง\n",
    "    เพื่อความแม่นยำสูงสุด\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        # นับ token ของคำปัจจุบันโดยใช้ tokenizer ของ Typhoon\n",
    "        word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        word_token_count = len(word_tokens)\n",
    "\n",
    "        # ถ้ารวมคำนี้แล้วเกิน limit ให้เริ่ม chunk ใหม่\n",
    "        if current_token_count + word_token_count > max_tokens_per_chunk:\n",
    "            if current_chunk_words:\n",
    "                chunks.append(\" \".join(current_chunk_words))\n",
    "                current_chunk_words = [word]\n",
    "                current_token_count = word_token_count\n",
    "            else:\n",
    "                # กรณีคำเดียวยาวเกิน limit (ควรหลีกเลี่ยง)\n",
    "                chunks.append(word)\n",
    "                current_chunk_words = []\n",
    "                current_token_count = 0\n",
    "        else:\n",
    "            current_chunk_words.append(word)\n",
    "            current_token_count += word_token_count\n",
    "\n",
    "    # เพิ่มชิ้นสุดท้าย\n",
    "    if current_chunk_words:\n",
    "        chunks.append(\" \".join(current_chunk_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- 3. ฟังก์ชันแปล ---\n",
    "def translate_chunks(chunks: list[str], target_lang: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    รับ list ของชิ้นส่วนข้อความ และแปลทีละชิ้น\n",
    "    \"\"\"\n",
    "    translated_chunks = []\n",
    "    system_prompt = f\"Translate the following text into {target_lang}.\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chunk},\n",
    "        ]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=8192,\n",
    "            temperature=0.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        translated_chunk = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        translated_chunks.append(translated_chunk.strip())\n",
    "\n",
    "    return translated_chunks\n",
    "\n",
    "\n",
    "# --- 4. ฟังก์ชันหลัก ---\n",
    "def translate_text_with_chunking(text: str, source_lang: str, target_lang: str, max_tokens_per_chunk: int = 4000):\n",
    "    \"\"\"\n",
    "    ฟังก์ชันหลักที่ใช้งานทั้งหมด\n",
    "    max_tokens_per_chunk: แนะนำให้ตั้ง < 8192 เพื่อให้มีพื้นที่สำหรับ prompt\n",
    "    \"\"\"\n",
    "    print(f\"Original text length: {len(text)} characters.\")\n",
    "    print(f\"Splitting text into chunks with max {max_tokens_per_chunk} tokens each...\")\n",
    "\n",
    "    # ใช้ tokenizer ของ Typhoon ในการนับ token\n",
    "    chunks = chunk_text_by_token(text, max_tokens_per_chunk)\n",
    "    print(f\"Text split into {len(chunks)} chunk(s).\\n\")\n",
    "\n",
    "    print(f\"Translating from {source_lang} to {target_lang}...\")\n",
    "    translated_chunks = translate_chunks(chunks, target_lang, model, tokenizer)\n",
    "    final_translation = \"\".join(translated_chunks)  # รวมผลลัพธ์ทั้งหมด\n",
    "    print(\"Translation completed.\\n\")\n",
    "\n",
    "    # แสดงตัวอย่าง\n",
    "    print(\"--- Chunk Breakdown ---\")\n",
    "    for i, (orig, trans) in enumerate(zip(chunks, translated_chunks)):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Original: {orig[:100]}...\")  # แสดง 100 ตัวอักษรแรกเป็นตัวอย่าง\n",
    "        print(f\"Translated: {trans[:100]}...\")\n",
    "    print(\"\\n----------------------\")\n",
    "\n",
    "    print(f\"\\nFinal combined translation ({target_lang}):\")\n",
    "    print(final_translation)\n",
    "\n",
    "    return final_translation\n",
    "\n",
    "\n",
    "# --- ตัวอย่างการใช้งาน ---\n",
    "if __name__ == \"__main__\":\n",
    "    long_text = LN_part2\n",
    "\n",
    "    # แปลจากอังกฤษไปไทย\n",
    "    result_thai = translate_text_with_chunking(\n",
    "        text=long_text,\n",
    "        source_lang=\"English\",\n",
    "        target_lang=\"Thai\",\n",
    "        max_tokens_per_chunk=200  # ปรับค่าได้ตามต้องการ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc25f68",
   "metadata": {},
   "source": [
    "โมเดล google/gemma-3-4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "my_token = \"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\" # ใส่ token ของคุณที่นี่\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=my_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bcfb9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\accelerate\\utils\\modeling.py:1566: UserWarning: Current model requires 33282 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ba92f7639a47ae968239df21b6b74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bceedc491994ec590fe093f40269ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42274ca91ba348e6bdaeae557e717170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db38b2a642b54d698c289a59f3ebd841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9df220651d4706a4814f35ac9f2cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc05a8f7c2742e383a5e2e57194529c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a642546642324109baab5413e8f66459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8aa1519d43481b8ae899ad8fca7554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862d796493a84384998bfc11e1c9a974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827dd2b614c048ee84d97f91c0e4701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 1. Login และตั้งค่า Model\n",
    "HF_TOKEN = \"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\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "def translate_with_context(long_text, chunk_size=2000):\n",
    "    \"\"\"\n",
    "    long_text: ข้อความภาษาอังกฤษยาวๆ\n",
    "    chunk_size: จำนวนตัวอักษรต่อรอบ (ปรับได้ตามความเหมาะสม)\n",
    "    \"\"\"\n",
    "    # แบ่งข้อความออกเป็นส่วนๆ\n",
    "    import textwrap\n",
    "    chunks = textwrap.wrap(long_text, width=chunk_size, break_long_words=False, replace_whitespace=False)\n",
    "    \n",
    "    full_translation = []\n",
    "    # สร้าง 'memory' เก็บประวัติการแปล (System Instruction + Previous Work)\n",
    "    chat_history = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a professional translator. Translate English to Thai naturally. Keep the tone consistent and remember specialized terms used in previous parts.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I understand. I will maintain consistency and natural Thai flow throughout the translation. Please provide the first part.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"--- Translating Part {i+1}/{len(chunks)} ---\")\n",
    "        \n",
    "        # เพิ่มข้อความส่วนใหม่เข้าไปใน history\n",
    "        chat_history.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Translate this part. Maintain consistency with previous parts if any:\\n\\n{chunk}\"\n",
    "        })\n",
    "\n",
    "        # เตรียม Input โดยใช้ Chat Template (ซึ่งจะรวมประวัติทั้งหมด)\n",
    "        prompt = processor.apply_chat_template(chat_history, add_generation_prompt=True, tokenize=False)\n",
    "        inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate คำแปล\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2048, # ปรับตามความเหมาะสมของแต่ละ Chunk\n",
    "                do_sample=True,\n",
    "                temperature=0.2, # ใช้ค่าต่ำเพื่อให้แปลแม่นยำ\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        # แกะเฉพาะคำตอบล่าสุดออกมา\n",
    "        response = processor.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Done part {i+1}\")\n",
    "        full_translation.append(response)\n",
    "\n",
    "        # เพิ่มคำแปลเข้าไปใน history เพื่อให้รอบหน้าโมเดลเห็นว่า 'แปลอะไรไปแล้ว'\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        # (Optional) ป้องกันประวัติยาวเกินไปจนแรมเต็ม \n",
    "        # แม้ Gemma 3 รับได้ 128K แต่ถ้า VRAM คุณน้อย อาจต้องเก็บแค่ 3-5 ส่วนล่าสุด\n",
    "        if len(chat_history) > 10: \n",
    "            chat_history = [chat_history[0], chat_history[1]] + chat_history[-6:]\n",
    "\n",
    "    return \"\\n\".join(full_translation)\n",
    "\n",
    "# # --- วิธีใช้งาน ---\n",
    "# english_document = \"\"\"ใส่ข้อความภาษาอังกฤษยาวๆ ของคุณที่นี่...\"\"\"\n",
    "# result = translate_with_context(english_document)\n",
    "\n",
    "# # บันทึกลงไฟล์\n",
    "# with open(\"translated_thai.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94953fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Translating Part 1/5 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- วิธีใช้งาน ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m english_document \u001b[38;5;241m=\u001b[39m LN_part2\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_with_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_document\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# บันทึกลงไฟล์\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_thai.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[16], line 55\u001b[0m, in \u001b[0;36mtranslate_with_context\u001b[1;34m(long_text, chunk_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Generate คำแปล\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 55\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     57\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;66;03m# ปรับตามความเหมาะสมของแต่ละ Chunk\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;66;03m# ใช้ค่าต่ำเพื่อให้แปลแม่นยำ\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# แกะเฉพาะคำตอบล่าสุดออกมา\u001b[39;00m\n\u001b[0;32m     64\u001b[0m response \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m][inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\generation\\utils.py:2625\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2618\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2619\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2620\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2622\u001b[0m     )\n\u001b[0;32m   2624\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2625\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2626\u001b[0m         input_ids,\n\u001b[0;32m   2627\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2628\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2629\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2630\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2631\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2632\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2633\u001b[0m     )\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2638\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2639\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2640\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2642\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\generation\\utils.py:3606\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3603\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[1;32m-> 3606\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3607\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:1085\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1081\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1082\u001b[0m )\n\u001b[0;32m   1083\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1085\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1086\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1087\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   1088\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1089\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1090\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1091\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1092\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1093\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1094\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1095\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1096\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1097\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1098\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs,\n\u001b[0;32m   1100\u001b[0m )\n\u001b[0;32m   1102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:939\u001b[0m, in \u001b[0;36mGemma3Model.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **lm_kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     causal_mask_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_causal_mask(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs),\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_sliding_window_causal_mask(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs),\n\u001b[0;32m    937\u001b[0m     }\n\u001b[1;32m--> 939\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[0;32m    940\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping,\n\u001b[0;32m    941\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    942\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    943\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    944\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    945\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    946\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    947\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    948\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs,\n\u001b[0;32m    950\u001b[0m )\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ModelOutputWithPast(\n\u001b[0;32m    953\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[0;32m    954\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    957\u001b[0m     image_hidden_states\u001b[38;5;241m=\u001b[39mimage_features \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    958\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:566\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    564\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 566\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    567\u001b[0m     hidden_states,\n\u001b[0;32m    568\u001b[0m     position_embeddings_global\u001b[38;5;241m=\u001b[39mposition_embeddings_global,\n\u001b[0;32m    569\u001b[0m     position_embeddings_local\u001b[38;5;241m=\u001b[39mposition_embeddings_local,\n\u001b[0;32m    570\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[0;32m    571\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    572\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    573\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    574\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    575\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    577\u001b[0m )\n\u001b[0;32m    579\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:385\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[1;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[1;32m--> 385\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    386\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    387\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    388\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    389\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    390\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    391\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    392\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    393\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    395\u001b[0m )\n\u001b[0;32m    396\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m    397\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:311\u001b[0m, in \u001b[0;36mGemma3Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    309\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 311\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    312\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    313\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aungl\\anaconda3\\envs\\Team_Project2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- วิธีใช้งาน ---\n",
    "english_document = LN_part2\n",
    "result = translate_with_context(english_document)\n",
    "\n",
    "# บันทึกลงไฟล์\n",
    "with open(\"translated_thai.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team_Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
